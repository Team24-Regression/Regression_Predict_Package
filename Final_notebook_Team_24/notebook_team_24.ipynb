{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sendy Logistics Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://cdn1.vc4a.com/media/2015/12/Sendy-delivery-900x322.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Logistics is fundamental to the success of a business while efficient and affordable logistics are a vital component of economic development. Like any logistics company, Sendy aims to improve the efficiency of businesses by providing faster delivery of their products.\n",
    "\n",
    "### About Sendy:\n",
    "Sendy is a business-to-business platform established in 2014, to enable businesses of all types and sizes to transport goods more efficiently across East Africa. It was headquartered in Kenya and has gradually expanded across the country and East African borders where  the it is enabling businesses to move large volumes of goods.\n",
    "\n",
    "### Aim and objectives:\n",
    "The aim of this notebook is to help Sendy improve their logistics and communicate an accurate arrival time to their customers. This is done through building a prediction model that estimates time of delivery of orders, from the point of driver pickup to the point of arrival at final destination. The model will enhance customer communication and improve the reliability of Sendy's services; which will ultimately improve customer experience. In addition, the solution from the model will enable Sendy to realise cost savings, and ultimately reduce the cost of doing business, through improved resource management and planning for order scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "# data processing\n",
    "import pandas as pd \n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn import tree\n",
    "\n",
    "#Varrible selection\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the csv files are converted to panda dataframe and are renamed.\n",
    "Train_df= pd.read_csv(\"Train.csv\")\n",
    "Test_df= pd.read_csv(\"Test.csv\")\n",
    "Riders_df= pd.read_csv(\"Riders.csv\")\n",
    "VariableDefinitions_df= pd.read_csv(\"VariableDefinitions.csv\")\n",
    "SampleSubmission_df= pd.read_csv(\"SampleSubmission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring data sets helps in developing deep understanding about the data, the data set is Explored with imported libraries.The Variable dataframe below displays the name of features and desciptions on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VariableDefinitions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe above is the list of the features with a short description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VariableDefinitions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data set\n",
    "Train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training-set has 21201 entries and 28 features + the target variable (Time from Pickup to Arrival). 6 of the features are floats, 13 are integers and 10 are objects.Since there is a Train data set, this implies that the data has been split and any aditing that will be done on the train data set will also be applied on the Test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above,few things can be noted. some of the features need to be converted into numeric ones, so that the machine learning algorithms can process them. Furthermore. Some more features, that contain missing values (NaN = not a number) can also be spoted, that would be deal with later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are all features present in the Train data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Rider data set\n",
    "Riders_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rider_df displays the information about the motorbike riders. It consist of 5 features and 960 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Riders_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the overview of the Riders data frame. The feature (Rider Id) is common to both Riders and Train Data Frame this allows for a merge between the two data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features found within the Rider data set.\n",
    "Riders_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #a look at the submission sample data frame\n",
    "SampleSubmission_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample submission displays the format in which sumbmision would be made for this predict on Zindi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __DATA PREPROCESSING__\n",
    "\n",
    "### Data cleaning and formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Joining the riders to both Train and Test data set\n",
    "Train_df = pd.merge(Train_df, Riders_df, left_on='Rider Id', \n",
    "                    right_on='Rider Id', how='left')\n",
    "Test_df = pd.merge(Test_df, Riders_df, left_on='Rider Id',\n",
    "                    right_on='Rider Id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and Train data frames are merged with Riders data data frame on the Rider Id feature because the features in the Rider data frame may have influence on the predict variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the new merged Train data set and The same is be done for the Test data set. For the purpose of this predict, the Vehicle Type feature is dropped because only motor bikes are considered. The naming format is not consistant in the above data frame(under scores will be use instead of spaces) and Features User Id, Rider ID and Order number will be dropped but the Order No from the Test df will not be dropped since it is require dor the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', Train_df['Rider Id'].nunique(), \n",
    "      'Motorbike riders', 'and', Train_df['User Id'].nunique(), \n",
    "      'Customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Formatting the naming of the columns\n",
    "Train_df.columns = Train_df.columns.str.replace(' ', '_')\n",
    "Test_df.columns = Test_df .columns.str.replace(' ', '_')\n",
    "\n",
    "    #removing \"-\" from the feature labels.\n",
    "Train_df.columns = Train_df.columns.str.replace('_-_', '_')\n",
    "Test_df.columns = Test_df .columns.str.replace('_-_', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordin to data from Sendy; there is a missing time features 'Arrival at Destination Times' in  the Test df and it can not be culculated because the Time_from_Pickup_to_Arrival.As a result,  Arrival at Destination Times will all be dropped aswel as the Vehicle type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df = Train_df.drop(['Vehicle_Type', \n",
    "                          'Arrival_at_Destination_Day_of_Month',\n",
    "                          'Arrival_at_Destination_Weekday_(Mo_=_1)',\n",
    "                          'Arrival_at_Destination_Time'], axis = 1)\n",
    "Test_df = Test_df.drop(['Vehicle_Type'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Handling missing data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aval_df = Train_df.notnull().sum(axis=0).reset_index()\n",
    "aval_df.columns = ['column_name', 'missing_count']\n",
    "aval_df = aval_df.loc[aval_df['missing_count']>0]\n",
    "aval_df = aval_df.sort_values(by='missing_count')\n",
    "\n",
    "ind = np.arange(aval_df.shape[0])\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "rects = ax.barh(ind, aval_df.missing_count.values, color='magenta')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(aval_df.column_name.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Total Count of Available values\")\n",
    "ax.set_ylabel(\"Features\")\n",
    "ax.set_title(\"Available Data In Each Column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above display how much data is available, the full bars means the data for that feature is comple and the last two bar bars are the only ones with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__More detailed look at what data is actually missing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Train_df missing data\n",
    "total = Train_df.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = Train_df.isnull().sum()/Train_df.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual Display of missing Data\n",
    "ax = missing_data['Total'].plot(kind='barh', figsize=(10, 15), color='#86bf91', zorder=2, width=0.68)\n",
    "    # Set x-axis label\n",
    "ax.set_xlabel(\"Total count of missing values\", labelpad=20, weight='bold', size=12)\n",
    "\n",
    "  # Set y-axis label\n",
    "ax.set_ylabel(\"Features\", labelpad=20, weight='bold', size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As obseved from both missing_data frame and the visual display above, it will be a bit tricky to deal with the 'Temparature' feature, which has 4366 missing values. However on the other hand, the 'Precipitation in mm' feature needs further investigation, but it looks like it would be better if it is dropped off from the dataset, since 97.4 % of it is missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Investigating on Precipitation in millimeter feature.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Checking if there are 0 mm records of Precipitation from both Test and Train data frame\n",
    "(Train_df['Precipitation_in_millimeters']==0).all(),(Test_df['Precipitation_in_millimeters']==0).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are no Recorded 0 values for precipitation in both of the data frames. It is not likely that pricipitation may occur in all days of the month. So it makes sense that 97% of the time in a month there is no precipitation, therefore the NaN values will be replaced with 0 values.Meaning on those days there was no precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #replacing NAN with 0 for precipitation feature\n",
    "Train_df[\"Precipitation_in_millimeters\"] = Train_df[\"Precipitation_in_millimeters\"].fillna(0)\n",
    "Test_df[\"Precipitation_in_millimeters\"] = Test_df[\"Precipitation_in_millimeters\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for relatinship between Temparature and features which may have influence of Time from Pickup to Arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Train_df data set \n",
    "Temp = sns.boxplot(x=\"Platform_Type\", y=\"Temperature\", hue=\"Personal_or_Business\",\n",
    "                 data=Train_df, palette=\"Greens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whisker and the box diagram propose that there is a relationship between the temparature and the platform as well as the Personal or bussiness feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #repacing the missing temparature with the mean\n",
    "Train_df = Train_df.fillna(Train_df.mean())\n",
    "Test_df = Test_df.fillna(Test_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will affect only the Temparature since it is the onlyone with the missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Data formating*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Four features containg time data but as an object:\n",
    "\n",
    "- Placement_Time\n",
    "- Confirmation - Time\n",
    "- Arrival_at_Pickup_Time\n",
    "- Pickup_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_time(df):\n",
    "    time_matrix = ['Placement_Time','Confirmation_Time', \n",
    "                   'Arrival_at_Pickup_Time', 'Pickup_Time']\n",
    "    for i in time_matrix:\n",
    "        df[i] = pd.to_datetime(df[i]).dt.strftime('%H:%M:%S')\n",
    "        df[i] = pd.to_timedelta(df[i])\n",
    "        df[i] = df[i].dt.total_seconds()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df = alter_time(Train_df)\n",
    "Test_df = alter_time(Test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df['Placement_to_Confiration_Time'] = Train_df['Confirmation_Time'] - Train_df['Placement_Time']\n",
    "Test_df['Placement_to_Confiration_Time'] = Test_df['Confirmation_Time'] - Test_df['Placement_Time'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new time features from the existing ones by culculating the difference from each.\n",
    "- Placement_Time\n",
    "- Confirmation - Time\n",
    "- Arrival_at_Pickup_Time\n",
    "- Pickup_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df['Placement_to_Confiration_Time'] = Train_df['Confirmation_Time'] - Train_df['Placement_Time']\n",
    "Test_df['Placement_to_Confiration_Time'] = Test_df['Confirmation_Time'] - Test_df['Placement_Time'] \n",
    "\n",
    "Train_df['Placement_to_Arrival_at_Pickup_Time'] = Train_df['Arrival_at_Pickup_Time'] - Train_df['Placement_Time']\n",
    "Test_df['Placement_to_Arrival_at_Pickup_Time'] = Test_df['Arrival_at_Pickup_Time'] - Test_df['Placement_Time'] \n",
    "\n",
    "Train_df['Placement_to_Pickup_Time'] = Train_df['Pickup_Time'] - Train_df['Placement_Time']\n",
    "Test_df['Placement_to_Pickup_Time'] = Test_df['Pickup_Time'] - Test_df['Placement_Time'] \n",
    "\n",
    "Train_df['Confirmation_to_Arrival_at_Pickup_Time'] = Train_df['Arrival_at_Pickup_Time'] - Train_df['Confirmation_Time']\n",
    "Test_df['Confirmation_to_Arrival_at_Pickup_Time'] = Test_df['Arrival_at_Pickup_Time'] - Test_df['Confirmation_Time'] \n",
    "\n",
    "Train_df['Confirmation_to_Pickup_Time'] = Train_df['Confirmation_Time'] - Train_df['Placement_Time']\n",
    "Test_df['Confirmation_to_Pickup_Time'] = Test_df['Confirmation_Time'] - Test_df['Placement_Time'] \n",
    "\n",
    "Train_df['Arrival_at_Pickup_to_Pickup_Time'] = Train_df['Confirmation_Time'] - Train_df['Placement_Time']\n",
    "Test_df['Arrival_at_Pickup_to_Pickup_Time'] = Test_df['Confirmation_Time'] - Test_df['Placement_Time'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a feature from Coordinates\n",
    "\n",
    "The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes.\n",
    "Below is the fuction that will calculate the disdance between the pickup and destination location in Kilo meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_vectorize(lon1, lat1, lon2, lat2): \n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2]) \n",
    "    newlon = lon2 - lon1\n",
    "    newlat = lat2 - lat1 \n",
    "    haver_formula = np.sin(newlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon/2.0)**2\n",
    " \n",
    "    dist = 2 * np.arcsin(np.sqrt(haver_formula ))\n",
    "    km = 6367 * dist #6367 for distance in KM(radius of the Earth)\n",
    "    return round(km, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_1 = haversine_vectorize(Train_df['Pickup_Lat'], \n",
    "                               Train_df['Pickup_Long'], \n",
    "                               Train_df['Destination_Lat'], \n",
    "                               Train_df['Destination_Long'])\n",
    "Train_df['Actual_Distance_KM'] = distance_1\n",
    "distance_2 = haversine_vectorize(Train_df['Pickup_Lat'], \n",
    "                               Train_df['Pickup_Long'], \n",
    "                               Train_df['Destination_Lat'], \n",
    "                               Train_df['Destination_Long'])\n",
    "Test_df['Actual_Distance_KM'] = distance_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the features in the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A first look at the behaviour of delivery time (Y - Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(range(Train_df.shape[0]), np.sort(Train_df['Time_from_Pickup_to_Arrival'].values), \n",
    "            color = 'orange')\n",
    "plt.xlabel('X - Variables', fontsize=12)\n",
    "plt.ylabel('Pickup to Arrival Time', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## *Thasamy will add interpretation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to look at the Platform Type and Business type to look at what makes up most of the orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding The Distribution Of The Data\n",
    "We are going to look at the features contained in the dataframe independently.This will help with understanding the distribution of the ranges and/or types contained in each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data\n",
    "\n",
    "A look at the Platform Type and Business or Personal features to see at what makes up most of the orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train_df['Platform_Type'].value_counts().plot(kind='pie')\n",
    "plt.axis('equal')\n",
    "plt.title('Platform Type contribution to total orders')\n",
    "plt.ylabel('')\n",
    "plt.show()\n",
    "\n",
    "ax=sns.countplot(x='Personal_or_Business', data=Train_df)\n",
    "ax.set(xlabel='Personal or Business', ylabel='Number of orders')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two figures, one can see that platfrom 3 has the highest numbers of orders and platform 4 has the least numbers. It can also be seen from the barchart that there are more orders for business than there are for personal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical data\n",
    "\n",
    "### Placement and confirmation Day of the month\n",
    "\n",
    "Placement and confirmation day of the month can be the same. If they are, it will save time that only one of the features is looked at. To check that, a new dataframe is created with rows that pass the condition that confirmation day is the same as placement day of the month. After it is created, if its length is still 21201 then this proves they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "New_df=Train_df[Train_df['Placement_Day_of_Month']!=Train_df['Confirmation_Day_of_Month']]\n",
    "New_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "New_df=Train_df[Train_df['Placement_Weekday_(Mo_=_1)']!=Train_df['Confirmation_Weekday_(Mo_=_1)']]\n",
    "New_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are two Business orders which were not confired on the same date and day of the month and week. !!!!!what do we do with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interptre the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(Train_df['Placement_Day_of_Month'],bins=4,kde=False, color='red')\n",
    "plt.xlabel('Day of the month')\n",
    "plt.ylabel('Number of orders')\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpretation chego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance\n",
    "\n",
    "The distance is looked at to see the range that contributes the most to the orders. The distance (Km) is divided into 5 ranges and is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(Train_df['Distance_(KM)'], bins=np.arange(0,60,10),kde=False, color='green')\n",
    "ax.set(xlabel='Distance(Km)', ylabel='Number of orders')\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Number of orders')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows that most of the orders are for a traveling distance of between 0km and 10km. Few orders are from a range 30km and 40km.\n",
    "\n",
    "### Temparature and Precipitation\n",
    "Now looking at both the temperature data and precipitation data, it can be seen that these are the only features with missing values. The graphs for boths are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2,figsize=(15,5))\n",
    "\n",
    "\n",
    "sns.distplot(Train_df['Temperature'],ax=axes[0], bins=np.arange(0,60,10),kde=False, color='orange')\n",
    "ax.set(xlabel='Temperature', ylabel='Number of orders')\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Number of orders')\n",
    "\n",
    "sns.distplot(Train_df['Precipitation_in_millimeters'],ax=axes[1], bins=np.arange(0,60,10),kde=False, color='pink')\n",
    "ax.set(xlabel='Precipitation in millimeters', ylabel='Number of orders')\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Number of orders')\n",
    "\n",
    "axes[0].set(xlabel=\"Temperature\", ylabel=\"Number of orders\")\n",
    "axes[1].set(xlabel=\"Precipitation in milimeters\", ylabel=\"Number of orders\")\n",
    "fig.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####interprt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(Train_df['Time_from_Pickup_to_Arrival'],bins=4,kde=False, color='red')\n",
    "plt.xlabel('Number of orders')\n",
    "plt.ylabel('Time from Pickup to Arrival')\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! chego how do you interprete the above.....nb Histogram are normaly used for categorigal data with numeric...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot to display the above histplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T_PA_no_orders = sns.scatterplot(y=\"Time_from_Pickup_to_Arrival\", x =\"No_Of_Orders\",  data=Train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the scatter show that the time from pickup to arival does not depend on no of orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!! the above count plot is hard to interprete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geography And Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ggplot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = sns.color_palette()\n",
    "ggplot(aes(x='Pickup_Lat', y='Pickup_Long', color= 'Time from Pickup to Arrival'), data=Train_df) + \\\n",
    "    geom_point() + \\\n",
    "    scale_color_gradient(low = 'red', high = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ggplot(aes(x='Destination_Lat', y='Destination_Long', color= 'Time_from_Pickup_to_Arrival'), data=Train_df) + \\\n",
    "    geom_point() + \\\n",
    "    scale_color_gradient(low = 'blue', high = 'yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "The distance of drop-offs are wider spread than pick up points, many destinations arrival time will be influenced by wider distances. We also notice that lengthy times for deliveries occur in both the central and outlier destinations. Traffic congestion and distance are all influential in the pick to arrival time duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the data types and the summary statistics of the variables are explored.\n",
    "Looking at the data types and number of entries of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Train_df.info() specifically outputs the number of non-null entries in each column. As such,It can be certain that the data has missing values if columns have a varying number of non-null entries.\n",
    "\n",
    "Below is the table showcasing the summary statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the means and standard deviations of different columns, The data will be standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variable Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The User ID, Order_No and Rider Id features will be dropped because they do not influence the Time_from_Pickup_to_Arrival, but the Order No feature will not be dropped from Test_df as it is required for subbmision.\n",
    "\n",
    "As it can be observed in the above table, the summary statistics being displayed is for numerical data at the moment. More importantly, all input data for regression model building purposes needs to be numerical. Therefore the text data Busines_or_Personal will be transformed  into numbers before training the machine learning model.\n",
    "\n",
    "To facilitate this transformation from textual-categorical data to numerical equivalents, pandas method called get_dummie will be used. The text data is categorical variable, and get_dummies will transform all the categorical text data into numbers by adding a column for each distinct category. The new column has a 1 for observations which were in this category, and a 0 for observations that were not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping User_Id, Order_No and Rider Id  Features\n",
    "Train_df = Train_df.drop(['Order_No', 'User_Id', 'Rider_Id'], axis = 1)\n",
    "Test_df = Test_df.drop(['User_Id', 'Rider_Id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(Train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(Train_df, drop_first=True)\n",
    "\n",
    "# Making sure that all the column names have correct format\n",
    "df_dummies.columns = [col.replace(\" \", \"_\") for col in df_dummies.columns]\n",
    "df_dummies.columns = [col.replace(\"(Mo_=_1)\",\"Mo_1\") for col in df_dummies.columns]\n",
    "df_dummies.columns = [col.replace(\"(KM)\",\"KM\") for col in df_dummies.columns]\n",
    "# Test_df\n",
    "Test_df.columns = [col.replace(\" \", \"_\") for col in Test_df.columns]\n",
    "Test_df.columns = [col.replace(\"(Mo_=_1)\",\"Mo_1\") for col in Test_df.columns]\n",
    "Test_df.columns = [col.replace(\"(KM)\",\"KM\") for col in Test_df.columns]\n",
    "\n",
    "# Reorder columns with the dependent variable (claim_amount) the last column\n",
    "column_titles = [col for col in df_dummies.columns if col !=\n",
    "                 'Time_from_Pickup_to_Arrival'] + ['Time_from_Pickup_to_Arrival']\n",
    "df_dummies = df_dummies.reindex(columns=column_titles)\n",
    "\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations and Model Structure\n",
    "\n",
    "Using the dummy variable dataframe, a model that predictsTime from Pickup to Arrival (the dependent variable) as a function of 33 different independent variables (IVs) can be build.\n",
    "\n",
    "Before this can be done, however, its better to reorder columns so that the dependent variable is the last column of the dataframe. This will make a heatmap visualisation representing a correlation matrix of the data easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_titles = [col for col in df_dummies.columns if col!= 'Time_from_Pickup_to_Arrival'] + ['Time_from_Pickup_to_Arrival']\n",
    "df_dummies=df_dummies.reindex(columns=column_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15));\n",
    "ax = fig.add_subplot(111);\n",
    "plot_corr(df_dummies.corr(), xnames = df_dummies.corr().columns, ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all of these variables are to be used, it would be like effectively working with more than enough information. The model will also have collinearity issues:\n",
    "\n",
    "Peronal_or_Business_Personal and peronal_or_Business_Business are perfectly negative correlated. All the features having blue squeres are also perfectly negative correlated.\n",
    "This will likely be a problem when building a model.\n",
    "\n",
    "checking what an OLS model summary says."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model using statsmodels.OLS\n",
    "\n",
    "#### Generating the regression string\n",
    "Following the process initially detailed in the Multiple Linear Regression Pt 2 - Checking Model Quality train, OLS model will be build and the model summary will be printed:\n",
    "\n",
    "y ~ X\n",
    "\n",
    "which is read as follows: \"Regress y on X\". statsmodels works in a similar way, so an appropriate string need to be generated to feed to the method in case it is required to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model DataFrame with all of the columns:\n",
    "dfm = df_dummies.copy()\n",
    "\n",
    "# The dependent variable:\n",
    "y_name = 'Time_from_Pickup_to_Arrival'\n",
    "# The independent variable\n",
    "# (let's first try all of the columns in the model DataFrame)\n",
    "X_names = [col for col in dfm.columns if col != y_name]\n",
    "\n",
    "# Build the OLS formula string \" y ~ X \"\n",
    "formula_str = y_name+\" ~ \"+\" + \".join(X_names);\n",
    "print('Formula:\\n\\t {}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model using the model dataframe\n",
    "model=ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Output the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is warning about strong multicollinearity. This is likely as a result of the incorrect filtering of one hot encoded dummy variables (It was noticed earlier that Peronal_or_Business_Personal and peronal_or_Business_Business are perfectly negative correlated).\n",
    "\n",
    "In order to ensure an underlying relationship between the categories is not assumed, pd.get_dummies is called with the argument drop_first=True so that only  n-1 columns are create for each variable with n categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(Train_df, drop_first=True)\n",
    "\n",
    "# Again make sure that all the column names have underscores instead of whitespaces\n",
    "df_dummies.columns = [col.replace(\" \", \"_\") for col in df_dummies.columns]\n",
    "df_dummies.columns = [col.replace(\"(Mo_=_1)\",\"Mo_1\") for col in df_dummies.columns]\n",
    "df_dummies.columns = [col.replace(\"(KM)\",\"KM\") for col in df_dummies.columns]\n",
    "\n",
    "# Reorder columns with the dependent variable (claim_amount) the last column\n",
    "column_titles = [col for col in df_dummies.columns if col !=\n",
    "                 'Time_from_Pickup_to_Arrival'] + ['Time_from_Pickup_to_Arrival']\n",
    "df_dummies = df_dummies.reindex(columns=column_titles)\n",
    "\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are 28 columns instead of 39. This gives for 28 potential independent variables that could be used to build a relationship on Time_from_Pickup_to_Arrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking what the OLS model summary would say if now only the 30 variable columns are fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model DataFrame will be kept, but only specify the columns wanted to fit this time\n",
    "X_names = [col for col in df_dummies.columns if col != y_name]\n",
    "\n",
    "# Build the OLS formula string \" y ~ X \"\n",
    "formula_str = y_name+' ~ '+'+'.join(X_names)\n",
    "\n",
    "# Fit the model using the model dataframe\n",
    "model = ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Output the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number has improved, but there is still mention of strong multicollinearity in warning [2]\n",
    "\n",
    "Making further selections on the variables now using their significance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection by Correlation and Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now there are 30 predictor variables to choose from, a way of guiding is needed to choose the best ones to be predictors. One way is to look at the correlations between the Time_from_Pickup_to_Arrival and each variables in the DataFrame and select those with the strongest correlations (both positive and negative).\n",
    "\n",
    "To consider how significant those features are.\n",
    "The code below will create a new DataFrame and store the correlation coefficents and p-values in that DataFrame for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating correlations between predictor variables and the response variable\n",
    "corrs = df_dummies.corr()['Time_from_Pickup_to_Arrival'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a dictionary of correlation coefficients and p-values\n",
    "dict_cp = {}\n",
    "\n",
    "column_titles = [col for col in corrs.index if col!= 'Time_from_Pickup_to_Arrival']\n",
    "for col in column_titles:\n",
    "    p_val = round(pearsonr(df_dummies[col], df_dummies['Time_from_Pickup_to_Arrival'])[1],6)\n",
    "    dict_cp[col] = {'Correlation_Coefficient':corrs[col],\n",
    "                    'P_Value':p_val}\n",
    "\n",
    "df_cp = pd.DataFrame(dict_cp).T\n",
    "df_cp_sorted = df_cp.sort_values('P_Value')\n",
    "df_cp_sorted[df_cp_sorted['P_Value']<0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the sorted list of the p-values and correlation coefficients for each of the features, when considered on their own.\n",
    "\n",
    "If a logic test was to be used with a significance value of 5% (p-value < 0.05),it would imply that the following features are statistically significant:\n",
    "- Distance_KM\n",
    "- Average_Rating\n",
    "- the Coordinates (pick long and lat, dest long and lat)\n",
    "- No-of_Orders\n",
    "- Arrival_at_Destination_time\n",
    "- Actual_Distance\n",
    "- Placement_to_Destination_Time\n",
    "- Arrived_at_Pickup_Time\n",
    "- Placement_Day_od_Month\n",
    "- Confirmation_Day_of_Month\n",
    "- Pickup_Day_Of_Month\n",
    "- Arrival_at_Pickup_Day_of_Month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the variables that have a significant correlation with the dependent variable are kept.they will be put into an independent variable DataFrame X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dependent variable remains the same:\n",
    "y_data = df_dummies[y_name]  # y_name = 'Time_from_Pickup_to_Arrival'\n",
    "\n",
    "# Model building - Independent Variable (IV) DataFrame\n",
    "X_names = list(df_cp[df_cp['P_Value'] < 0.05].index)\n",
    "X_data = df_dummies[X_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid autocorrelation,it is needed to look for predictor variable pairs which have a high correlation with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the correlation matrix\n",
    "corr = X_data.corr()\n",
    "\n",
    "# Find rows and columnd where correlation coefficients > 0.9 or <-0.9\n",
    "corr[np.abs(corr) > 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at the whole correlation matrix, it might be easier to isolate the sections of the correlation matrix to where the off-diagonal correlations are high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just like before, the correlation matrix is created\n",
    "# rows and columnd where correlation coefficients > 0.9 or <-0.9\n",
    "corr = X_data.corr()\n",
    "r, c = np.where(np.abs(corr) > 0.9)\n",
    "\n",
    "# The interest is in the off diagonal entries:\n",
    "off_diagonal = np.where(r != c)\n",
    "\n",
    "# Showing the correlation matrix rows and columns where there is  highly correlated off diagonal entries:\n",
    "corr.iloc[r[off_diagonal], c[off_diagonal]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it looks like six of the features are very highly correlated.\n",
    "\n",
    "This is also visible looking back at the correlation coefficient heatmap and matrix from earlier, but a more focused / subset view of the matrix is useful to isolate the coefficients of interest.\n",
    "\n",
    "Distance is slightly better correlated (and lower p-value) to the dependent variable\n",
    "there fore; Arrival_at_Pickip_day_of_Month, Pickup_Day_of_Month, Confirmation_Day_of_Month and Placement_Day_of_Month are dropped form the feature dataframe. \n",
    "The resulting OLS fit summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new subset of potential independent variables\n",
    "X_remove = ['Arrival_at_Pickip_day_of_Month', 'Pickup_Day_of_Month', 'Confirmation_Day_of_Month', 'Placement_Day_of_Month']\n",
    "X_corr_names = [col for col in X_names if col not in X_remove]\n",
    "\n",
    "# Creating new OLS formula based-upon the smaller subset\n",
    "formula_str = y_name+' ~ '+' + '.join(X_corr_names);\n",
    "print('Formula:\\n\\t{}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the OLS model using the model dataframe\n",
    "model=ols(formula=formula_str, data=dfm)\n",
    "fitted = model.fit()\n",
    "\n",
    "# Display the fitted summary\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Selection by Variance Thresholds\n",
    "\n",
    "Variance Thresholds remove features whose values don't change much from observation to observation. The objective here is to remove all features that have a variance lower than the selected threshold.\n",
    "\n",
    "It is important to note that variance is dependent on scale, so the features will have to be normalized before implementing variance thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating data into independent (X) and independent (y) variables\n",
    "X_names = list(df_dummies.columns)\n",
    "X_names.remove(y_name)\n",
    "X_data = df_dummies[X_names]\n",
    "y_data = df_dummies[y_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "X_normalize = pd.DataFrame(X_scaled, columns=X_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold in Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Variance Threshold in Scikit Learn:\n",
    "\n",
    "Import and create an instance of the VarianceThreshold class;\n",
    "Using the .fit() method to select subset of features based on the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VarianceThreshold object\n",
    "selector = VarianceThreshold(threshold=0.03)\n",
    "\n",
    "# Using the object to apply the threshold on data\n",
    "selector.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variance Threshold has been applied to the data.Now a closer look at the calculated variance for each predictive variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting column variances\n",
    "column_variances = selector.variances_\n",
    "\n",
    "vars_dict = {}\n",
    "vars_dict = [{\"Variable_Name\": c_name, \"Variance\": c_var}\n",
    "             for c_name, c_var in zip(X_normalize.columns, column_variances)]\n",
    "df_vars = pd.DataFrame(vars_dict)\n",
    "df_vars.sort_values(by='Variance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows the variances of the individual columns before any threshold is applied. It allows anyone to revise  initial variance threshold if there might be a need to exclude important variables.\n",
    "\n",
    "The results needs to be extracted and used to select new columns - which form a subset of all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting new columns\n",
    "X_new = X_normalize[X_normalize.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Saving variable names for later\n",
    "X_var_names = X_new.columns\n",
    "\n",
    "# Viewing first few entries\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a threshold of 0.03, the predictor number has gone from 29 to 12 predictors.\n",
    "\n",
    "Trying few more few more thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Variance Threshold objects\n",
    "selector_1 = VarianceThreshold(threshold=0.05)\n",
    "selector_2 = VarianceThreshold(threshold=0.1)\n",
    "selector_3 = VarianceThreshold(threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_1.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_2.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_3.fit(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X_normalize[X_normalize.columns[selector_1.get_support(indices=True)]]\n",
    "X_2 = X_normalize[X_normalize.columns[selector_2.get_support(indices=True)]]\n",
    "X_3 = X_normalize[X_normalize.columns[selector_3.get_support(indices=True)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " graphing the number of predictors by the thresholds to investigate the relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 3), nrows=1, ncols=1)\n",
    "\n",
    "# Create list of titles and predictions to use in for loop\n",
    "subset_preds = [X_1.shape[1], X_2.shape[1], X_3.shape[1]]\n",
    "thresholds = ['0.05', '0.1', '0.15']\n",
    "\n",
    "# Plot graph\n",
    "ax.set_title('# of Predictors vs Thresholds')\n",
    "ax.set_ylabel('# of Predictors')\n",
    "ax.set_xlabel('Threshold')\n",
    "sns.barplot(x=thresholds, y=subset_preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an increase the threshold, the number of dimensions decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking what the resulting OLS fit summary for a threshold of 0.03 says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new OLS formula\n",
    "formula_str = y_name+' ~ '+' + '.join(X_new.columns)\n",
    "print('Formula:\\n\\t{}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model using the model dataframe\n",
    "model = ols(formula=formula_str, data=df_dummies)\n",
    "fitted = model.fit()\n",
    "\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modelling the y variable\n",
    "Now that the DataFrame has been shortened using various methods, Its time to see if  linear regression models can be fit and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "Assuring that all models are trained and tested on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                    y_data,\n",
    "                                                    test_size=0.20,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data for variance threshold model\n",
    "X_var_train = X_train[X_var_names]\n",
    "X_var_test = X_test[X_var_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data for correlation threshold model\n",
    "X_corr_train = X_train[X_corr_names]\n",
    "X_corr_test = X_test[X_corr_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting models\n",
    "instantiate and fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm_corr = LinearRegression()\n",
    "lm_var = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train);\n",
    "lm_corr.fit(X_corr_train,y_train);\n",
    "lm_var.fit(X_var_train,y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing model accuracy\n",
    "checking how the linear models performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating figure and axes\n",
    "f, ax = plt.subplots(figsize=(15, 5), nrows=1, ncols=3, sharey=True)\n",
    "\n",
    "# Creating list of titles and predictions to use in for loop\n",
    "train_pred = [lm.predict(X_train),\n",
    "              lm_corr.predict(X_corr_train),\n",
    "              lm_var.predict(X_var_train)]\n",
    "test_pred = [lm.predict(X_test),\n",
    "             lm_corr.predict(X_corr_test),\n",
    "             lm_var.predict(X_var_test)]\n",
    "title = ['No threshold', 'Corr threshold', 'Var threshold']\n",
    "\n",
    "# Key:\n",
    "# No threshold - linear regression with all predictive variables\n",
    "# Corr threshold - linear regression with correlation thresholded predictive variables\n",
    "# Var threshold - linear regression with variance thresholded predictive variables\n",
    "\n",
    "\n",
    "# Loop through all axes to plot each model's results\n",
    "for i in range(3):\n",
    "    test_mse = round(mean_squared_error(test_pred[i], y_test), 4)\n",
    "    test_r2 = round(r2_score(test_pred[i], y_test), 4)\n",
    "    train_mse = round(mean_squared_error(train_pred[i], y_train), 4)\n",
    "    train_r2 = round(r2_score(train_pred[i], y_train), 4)\n",
    "    title_str = f\"Linear Regression({title[i]}) \\n train MSE = {train_mse} \\n \" + \\\n",
    "                f\"test MSE = {test_mse} \\n training $R^{2}$ = {train_r2} \\n \" + \\\n",
    "                f\"test $R^{2}$ = {test_r2}\"\n",
    "    ax[i].set_title(title_str)\n",
    "    ax[i].set_xlabel('Actual')\n",
    "    ax[i].set_ylabel('Predicted')\n",
    "    ax[i].plot(y_test, y_test, 'r')\n",
    "    ax[i].scatter(y_test, test_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummy variables\n",
    "def encode_dummy(df, column_name):\n",
    "    df[column_name] =  pd.get_dummies(df[column_name],drop_first = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LASSO module\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create standardization object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LASSO model object, setting alpha to 0.01\n",
    "lasso = Lasso(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled,\n",
    "                                                    y_data,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=1,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LASSO model object, setting alpha to 0.01\n",
    "lasso = Lasso(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the LASSO model\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract intercept from model\n",
    "intercept = float(lasso.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract coefficient from model\n",
    "coeff = pd.DataFrame(lasso.coef_, Train_df.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = float(lasso.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract intercept\n",
    "print(\"Intercept:\", float(intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save standardized features into new variable\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train/test split module\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LASSO model object, setting alpha to 0.01\n",
    "lasso = Lasso(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = float(lasso.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = pd.DataFrame(lasso.coef_, X.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept:\", float(intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB : These functions return  the model and predicted y values for each model you call. For usage check code below the functions. A function for calculating RMSE was also added in the case we use the training data as our main data that we split into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_v_m(X, y, X_test):\n",
    "    regressor = SVR(kernel = 'rbf')\n",
    "    regressor.fit(X, y)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    return regressor, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X, y, X_test):\n",
    "    regressor = RandomForestRegressor(n_estimators = 5, random_state = 0)\n",
    "    regressor.fit(X, y)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    return regressor, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X, y, X_test):\n",
    "    regressor = DecisionTreeRegressor(random_state = 0)\n",
    "    regressor.fit(X, y)\n",
    "    #y_pred = regressor.predict(X_test)\n",
    "    return regressor, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_regression(X, y, X_test):\n",
    "    poly_reg = PolynomialFeatures(degree = 2)\n",
    "    X_poly = poly_reg.fit_transform(X)\n",
    "    lin_reg_2 = LinearRegression()\n",
    "    lin_reg_2.fit(X_poly, y)\n",
    "    y_pred = lin_reg_2.predict(X_test)\n",
    "    return lin_reg_2, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_linear(X, y, X_test):\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(X, y)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    return regressor, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_perfomance(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, y_pred = multi_linear(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753.5630641087931\n"
     ]
    }
   ],
   "source": [
    "print(RMSE_perfomance(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_name):\n",
    "    model_save_path = file_name+\".pkl\"\n",
    "    with open(model_save_path,'wb') as file:\n",
    "        pickle.dump(model,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_name):\n",
    "    model_load_path = file_name+\".pkl\"\n",
    "    with open(model_load_path,'rb') as file:\n",
    "        unpickled_model = pickle.load(file)\n",
    "    return unpickled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name  = 'saved_model_1'\n",
    "save_model(model, 'saved_model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(file_name)\n",
    "y_pred = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1410.7421875, 2959.8671875, 1662.7421875, ..., 2621.1171875,\n",
       "       1908.9921875, 1775.7421875])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
